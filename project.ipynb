{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOIL Project 4: Overcoming catastrophic forgetting in neural networks.\n",
    "\n",
    "[This paper](https://arxiv.org/pdf/1612.00796.pdf), by James Kirkpatrick et al., studies the setting of continual learning, where an agent learns multiple tasks sequentially, i.e., accumulating knowledge for a new task/experience without having access to data from past tasks. In this setting, the phenomenon of catastrophic forgetting arises; the performance of the agent on past experiences significantly diminishes upon learning new tasks. The paper proposes the use of a regularization term to combat this problem. We suggest steering from the reinforcement learning experience and, instead, focus on supervised learning. Reproduce the results of Figure 2 (subfigures A + B) on the following datasets: PermutedMNIST (as in the paper) as well as RotatedMNIST (ten tasks of rotating MNIST, where task i is produced by a fixed rotation of 10(i − 1) degrees). Compare the results of the proposed regularization with naive L2 regularization and no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data:\n",
    "\n",
    "Create three tasks of permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task = namedtuple(\"Task\", [\"name\", \"train\", \"test\"])\n",
    "\n",
    "def create_permuted_mnist(rng):\n",
    "    \"\"\"\n",
    "    Create a permuted MNIST dataset for a given task ID using a shared random number generator.\n",
    "    \"\"\"\n",
    "    perm = torch.tensor(rng.permutation(28 * 28), dtype=torch.long)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(-1)[perm].view(1, 28, 28))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Fix a single seed\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# Create permuted tasks\n",
    "Original_MNIST = Task(name=\"Original MNIST\", train=mnist_train, test=mnist_test)\n",
    "perm_tasks = [\n",
    "    Task(name=\"A: Permuted MNIST\", train=create_permuted_mnist(rng)[0], test=create_permuted_mnist(rng)[1]),\n",
    "    Task(name=\"B: Permuted MNIST\", train=create_permuted_mnist(rng)[0], test=create_permuted_mnist(rng)[1]),\n",
    "    Task(name=\"C: Permuted MNIST\", train=create_permuted_mnist(rng)[0], test=create_permuted_mnist(rng)[1])\n",
    "]\n",
    "\n",
    "for task in [Original_MNIST] + perm_tasks:\n",
    "    loader = DataLoader(task.train, batch_size=5, shuffle=False)\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "    for j in range(5):\n",
    "        axes[j].imshow(images[j].squeeze(0), cmap=\"gray\")\n",
    "        axes[j].set_title(f\"Label: {labels[j].item()}\")\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    fig.suptitle(f\"Samples from {task.name}\", fontsize=16)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create rotated MNIST datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotated_mnist(task_id, base_angle=0):\n",
    "    rotation_angle = base_angle + 10 * (task_id - 1)  # Fixed rotation for task\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: transforms.functional.rotate(x, angle=rotation_angle))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Generate ten tasks of Rotated MNIST\n",
    "rot_tasks = [\n",
    "    Task(\n",
    "        name=f\"Task {i}: Rotated MNIST {10 * (i - 1)}°\",\n",
    "        train=create_rotated_mnist(i)[0],\n",
    "        test=create_rotated_mnist(i)[1]\n",
    "    )\n",
    "    for i in range(1, 11)\n",
    "]\n",
    "\n",
    "# Visualize samples from each task\n",
    "for task in rot_tasks:\n",
    "    loader = DataLoader(task.train, batch_size=5, shuffle=False)\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 4))\n",
    "    for j in range(5):\n",
    "        axes[j].imshow(images[j].squeeze(0), cmap=\"gray\")\n",
    "        axes[j].set_title(f\"Label: {labels[j].item()}\")\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    fig.suptitle(f\"Rotated MNIST: Samples from {task.name}\", fontsize=16)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the model\n",
    "\n",
    "The paper proposes a fully connected network (FCN)\n",
    "\n",
    "They don't mention the size of the validation set, should we just take 20% ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fcn(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_size, dropout=False):\n",
    "        super(Fcn, self).__init__()\n",
    "        layers = [torch.nn.Flatten()] # Flatten the input tensor for fcn\n",
    "        \n",
    "        # MNIST image flattened vector to first hidden layer\n",
    "        layers.append(nn.Linear(28*28, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "        \n",
    "        # hidden layers \n",
    "        # NOTE: not entirely sure if this is what they ment by hidden layers, my interpretation is\n",
    "        # that they want the same number of fc layers here as they said in the paper + separate input and output layers...\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(p=0.5))\n",
    "        \n",
    "        # NOTE: maybe this layer should be removed and the last one of the above should already map to 10 classes? (I don't think so)\n",
    "        # output layer (map to 10 classes for MNIST)\n",
    "        layers.append(nn.Linear(hidden_size, 10))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def compute_fisher_information(self, dataloader, criterion):\n",
    "        \"\"\"Computes the diagonal Fisher Information Matrix (FIM).\"\"\"\n",
    "        fisher_matrix = {}\n",
    "        self.eval()\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            fisher_matrix[name] = torch.zeros_like(param)\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(next(self.parameters()).device), targets.to(next(self.parameters()).device)\n",
    "            self.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            for name, param in self.named_parameters():\n",
    "                fisher_matrix[name] += param.grad ** 2 / len(dataloader)\n",
    "        \n",
    "        for name in fisher_matrix:\n",
    "            self.fisher_info[name] = fisher_matrix[name].detach()\n",
    "            self.saved_params[name] = self.state_dict()[name].detach().clone()\n",
    "\n",
    "    def ewc_loss(self, lambda_ewc):\n",
    "        \"\"\"Computes the EWC regularization loss.\"\"\"\n",
    "        ewc_loss = 0.0\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.fisher_info:\n",
    "                ewc_loss += torch.sum(self.fisher_info[name] * (param - self.saved_params[name]) ** 2)\n",
    "        return lambda_ewc * ewc_loss            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function ot train the model\n",
    "# Add early stopping\n",
    "# also add loss as optimizer I think to be able to do EWC...\n",
    "def train_sequential_tasks(model, train_tasks, optimizer, num_epochs=10, early_stopping=False, l2_reg = 0.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_tasks = len(train_tasks)\n",
    "\n",
    "    train_loaders = [DataLoader(task.train, batch_size=64, shuffle=True) for task in train_tasks]\n",
    "\n",
    "    train_accuracies = np.zeros((n_tasks, n_tasks * num_epochs))\n",
    "\n",
    "    # Sequentially train on tasks\n",
    "    for i, train_loader in enumerate(train_loaders):\n",
    "        print(f\"Training on Task {i + 1}/{len(train_tasks)}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            # Training on current task\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if l2_reg > 0 and i > 0:\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                    loss += l2_reg * l2_norm\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Store current task accuracy for this epoch\n",
    "            current_task_accuracy = correct / total\n",
    "            train_accuracies[i, i * num_epochs + epoch] = current_task_accuracy\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\\t Accuracy: {current_task_accuracy:.4f} -> Evaluating on previous tasks...\", end=\" \")\n",
    "\n",
    "            # Evaluate performance on all previous tasks\n",
    "            for j, loader in enumerate(train_loaders[:i]):\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in loader:\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        total += labels.size(0)\n",
    "                task_accuracy = correct / total\n",
    "                train_accuracies[j, i * num_epochs + epoch] = task_accuracy\n",
    "\n",
    "            print(\"Finished!\")\n",
    "\n",
    "    return train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequential_tasks_ismail(model, train_tasks, optimizer, num_epochs=10, early_stopping=True, ewc=False, lambda_ewc = 15):\n",
    "    setting = \"EWC\" if ewc else \"Normal SGD\"\n",
    "    print(\"Setting of training: \" + setting)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    n_tasks = len(train_tasks)\n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "\n",
    "    # Split train datasets into train and validation sets\n",
    "    for task in train_tasks: \n",
    "        train_size = int(0.8 * len(task.train))  # 80% train, 20% validation\n",
    "        val_size = len(task.train) - train_size\n",
    "        train_subset, val_subset = random_split(task.train, [train_size, val_size])\n",
    "        train_loaders.append(DataLoader(train_subset, batch_size=64, shuffle=True))\n",
    "        val_loaders.append(DataLoader(val_subset, batch_size=64, shuffle=False))\n",
    "\n",
    "    train_accuracies = np.zeros((n_tasks, n_tasks * num_epochs))\n",
    "    val_accuracies = np.zeros((n_tasks, n_tasks * num_epochs))\n",
    "\n",
    "    best_model_state = None\n",
    "    best_avg_val_error = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 5\n",
    "\n",
    "    # Sequentially train on tasks\n",
    "    for i, train_loader in enumerate(train_loaders):\n",
    "        print(f\"Training on Task {i + 1}/{len(train_tasks)}\")\n",
    "        \n",
    "        if (i > 0) and ewc:\n",
    "            model.compute_fisher_information(train_loaders[i-1], criterion)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            # Training on current task\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                if (i > 0) and ewc:\n",
    "                    loss += model.ewc_loss(lambda_ewc)\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Store current task accuracy for this epoch\n",
    "            current_task_accuracy = correct_train / total_train\n",
    "            train_accuracies[i, i * num_epochs + epoch] = current_task_accuracy\n",
    "\n",
    "            # Validation on current task\n",
    "            model.eval()\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loaders[i]:\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "                    total_val += labels.size(0)\n",
    "            val_accuracy = correct_val / total_val\n",
    "            val_accuracies[i, i * num_epochs + epoch] = val_accuracy\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\\t Train Accuracy: {current_task_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f} -> Evaluating on previous tasks...\", end=\" \")\n",
    "\n",
    "            # Evaluate performance on all previous tasks (training)\n",
    "            for j, loader in enumerate(train_loaders[:i]):\n",
    "                correct_train_prev = 0\n",
    "                total_train_prev = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in loader:\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        correct_train_prev += (predicted == labels).sum().item()\n",
    "                        total_train_prev += labels.size(0)\n",
    "                train_accuracies[j, i * num_epochs + epoch] = correct_train_prev / total_train_prev\n",
    "            \n",
    "            # Evaluate performance on all previous tasks (validation)\n",
    "            for j, loader in enumerate(val_loaders[:i]):\n",
    "                correct_val_prev = 0\n",
    "                total_val_prev = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in loader:\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        correct_val_prev += (predicted == labels).sum().item()\n",
    "                        total_val_prev += labels.size(0)\n",
    "                val_accuracies[j, i * num_epochs + epoch] = correct_val_prev / total_val_prev            \n",
    "            \n",
    "            # NOTE: tell me what you think of the all tasks calculation ? Not sure about it \n",
    "            all_tasks_val_accuracy = np.mean(val_accuracies[:, i * num_epochs + epoch])\n",
    "            print(f\"Val Accuracy: {all_tasks_val_accuracy:.4f}, early_stopping_counter: {early_stopping_counter},  Finished!\")\n",
    "\n",
    "            # Early stopping condition based on validation error (all tasks)\n",
    "            # NOTE: my interpretation of \"Early stopping was implemented by computing the test error on the\n",
    "            # validation set for all pixel permutations\" is that we should use val_accuracies of all tasks\n",
    "            current_val_error = 1 - all_tasks_val_accuracy\n",
    "            if current_val_error < best_avg_val_error:\n",
    "                best_avg_val_error = current_val_error\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping and early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered, restoring best model state.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    return train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = Fcn(num_hidden_layers=2, hidden_size=400)\n",
    "\n",
    "summary(model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the training with normal SGD and see what happens, takes like 10-12 mins for me\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_accuracies_sgd, val_accuracies_sgd = train_sequential_tasks_ismail(model, perm_tasks, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increasing all tasks val accuracy is weird no ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with EWC\n",
    "model = Fcn(num_hidden_layers=2, hidden_size=400)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "lambda_ewc = 15  # Regularization strength\n",
    "\n",
    "train_accuracies_ewc, val_accuracies_ewc = train_sequential_tasks_ismail(model, perm_tasks, optimizer, num_epochs=num_epochs, early_stopping=True, ewc=True, lambda_ewc = lambda_ewc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clue how they manage to get that high accuracy for the first task that quickly in the paper. Maybe they train with original mnist first to get closer?? Also the results are not like suuuper close but ok-ish I think...\n",
    "\n",
    "### Try with L2 norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the training with L2 norm and see what happens\n",
    "\n",
    "model = Fcn(num_hidden_layers=2, hidden_size=400)\n",
    "# they don't specify the weight decay in the paper, so I just chose 1e-3, maybe we should increase...\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_accuracies_l2 = train_sequential_tasks(model, perm_tasks, optimizer, num_epochs=num_epochs, l2_reg = 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "+ EWC not really working \n",
    "\n",
    "+ L2 is looking like SGD should look like and when weight decay is 10-3 it overlaps with the other two curves \n",
    "\n",
    "+ Shouldn't we start regularizing with L2 after the first task only? Since the goal is to say in the region near optimal performance for the first task? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks, total_epochs = train_accuracies_sgd.shape\n",
    "fig, axes = plt.subplots(n_tasks, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "for i in range(n_tasks):\n",
    "  valid_epochs_ewc = np.where(train_accuracies_ewc[i] >= 0)[0]\n",
    "  axes[i].plot(valid_epochs_ewc, train_accuracies_ewc[i, valid_epochs_ewc], label=f\"EWC\", color = \"red\")\n",
    "\n",
    "  valid_epochs_l2 = np.where(train_accuracies_l2[i] >= 0)[0]\n",
    "  axes[i].plot(valid_epochs_l2, train_accuracies_l2[i, valid_epochs_l2], label=f\"L2\", color = \"green\")\n",
    "\n",
    "  valid_epochs_sgd = np.where(train_accuracies_sgd[i] >= 0)[0]\n",
    "  axes[i].plot(valid_epochs_sgd, train_accuracies_sgd[i, valid_epochs_sgd], label=f\"SGD\", color = \"blue\")\n",
    "\n",
    "  axes[i].vlines(np.arange(0, total_epochs, num_epochs), 0.2, 1.0, linestyles=\"--\", colors=\"gray\")\n",
    "  axes[i].set_ylabel(f\"Task {i + 1}\")\n",
    "  axes[i].legend(loc=\"upper right\", fontsize=10)\n",
    "  axes[i].set_ylim(0.3, 1.0)\n",
    "  axes[i].grid(alpha=0.4)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "fig.suptitle(\"Accuracy for permutedMNIST\", fontsize=14)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "\n",
    "plt.savefig(\"Figure_2A\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
